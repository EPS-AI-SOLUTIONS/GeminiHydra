import { cleanEnv, num, str } from 'envalid';

export const ENV = cleanEnv(process.env, {
  API_VERSION: str({ default: 'v1' }),
  DEFAULT_MODEL: str({ default: 'llama3.2:3b' }),
  FAST_MODEL: str({ default: 'llama3.2:1b' }),
  CODER_MODEL: str({ default: 'qwen2.5-coder:1.5b' }),
  CACHE_DIR: str({ default: './cache' }),
  CACHE_TTL: num({ default: 3600 }),
  CACHE_ENABLED: str({ default: 'true' }),
  CACHE_ENCRYPTION_KEY: str({ default: '' }),
  CACHE_MAX_ENTRY_BYTES: num({ default: 200000 }),
  CACHE_CLEANUP_INTERVAL_MS: num({ default: 300000 }),
  CACHE_MAX_TOTAL_MB: num({ default: 250 }),
  QUEUE_MAX_CONCURRENT: num({ default: 4 }),
  QUEUE_MAX_RETRIES: num({ default: 3 }),
  QUEUE_RETRY_DELAY_BASE: num({ default: 1000 }),
  QUEUE_TIMEOUT_MS: num({ default: 60000 }),
  QUEUE_RATE_LIMIT_TOKENS: num({ default: 10 }),
  QUEUE_RATE_LIMIT_REFILL: num({ default: 2 }),
  QUEUE_PERSISTENCE_PATH: str({ default: './cache/queue-state.json' }),
  QUEUE_PERSISTENCE_ENABLED: str({ default: 'false' }),
  MODEL_CACHE_TTL_MS: num({ default: 300000 }),
  GEMINI_MODELS_CACHE_TTL_MS: num({ default: 3600000 }),
  GEMINI_FETCH_TIMEOUT_MS: num({ default: 8000 }),
  GEMINI_FETCH_RETRIES: num({ default: 3 }),
  HEALTH_CHECK_TIMEOUT_MS: num({ default: 5000 }),
  PROMPT_MAX_LENGTH: num({ default: 20000 }),
  PROMPT_RISK_BLOCK: str({ default: 'false' }),
  MODEL_ALLOWLIST: str({ default: '' }),
  MODEL_DENYLIST: str({ default: '' }),
  AI_PROVIDER_FALLBACK: str({ default: 'anthropic,openai,google,mistral,groq,ollama' }),
  LOG_LEVEL: str({ default: 'info' })
});
