# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
GEMINI_API_KEY=

# Optional: Default model
# OLLAMA_MODEL=llama3.2:3b

# Hydra defaults
DEFAULT_MODEL=llama3.2:3b
FAST_MODEL=llama3.2:1b
CODER_MODEL=qwen2.5-coder:1.5b
API_VERSION=v1

# Cache configuration
CACHE_DIR=./cache
CACHE_TTL=3600
CACHE_ENABLED=true
# Max cache entry size in bytes
CACHE_MAX_ENTRY_BYTES=200000
# Cleanup interval in ms (0 disables cleanup)
CACHE_CLEANUP_INTERVAL_MS=300000
# Max total cache size in MB
CACHE_MAX_TOTAL_MB=250
# Provide a 32-byte key (hex or base64) for AES-256-GCM encryption
# CACHE_ENCRYPTION_KEY=

# Queue configuration
QUEUE_MAX_CONCURRENT=4
QUEUE_MAX_RETRIES=3
QUEUE_RETRY_DELAY_BASE=1000
QUEUE_TIMEOUT_MS=60000
QUEUE_RATE_LIMIT_TOKENS=10
QUEUE_RATE_LIMIT_REFILL=2
# Queue persistence
QUEUE_PERSISTENCE_ENABLED=false
QUEUE_PERSISTENCE_PATH=./cache/queue-state.json

# Model cache configuration
MODEL_CACHE_TTL_MS=300000
GEMINI_MODELS_CACHE_TTL_MS=3600000
GEMINI_FETCH_TIMEOUT_MS=8000
GEMINI_FETCH_RETRIES=3
PROMPT_MAX_LENGTH=20000
PROMPT_RISK_BLOCK=false
MODEL_ALLOWLIST=
MODEL_DENYLIST=
AI_PROVIDER_FALLBACK=anthropic,openai,google,mistral,groq,ollama

# Logging
LOG_LEVEL=info
